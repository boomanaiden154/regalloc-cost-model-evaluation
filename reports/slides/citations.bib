@inproceedings{abel_uica_2022,
	location = {New York, {NY}, {USA}},
	title = {{uiCA}: accurate throughput prediction of basic blocks on recent intel microarchitectures},
	isbn = {978-1-4503-9281-5},
	url = {https://doi.org/10.1145/3524059.3532396},
	doi = {10.1145/3524059.3532396},
	series = {{ICS} '22},
	shorttitle = {{uiCA}},
	abstract = {Performance models that statically predict the steady-state throughput of basic blocks on particular microarchitectures, such as {IACA}, Ithemal, llvm-mca, {OSACA}, or {CQA}, can guide optimizing compilers and aid manual software optimization. However, their utility heavily depends on the accuracy of their predictions. The average error of existing models compared to measurements on the actual hardware has been shown to lie between 9\% and 36\%. But how good is this? To answer this question, we propose an extremely simple analytical throughput model that may serve as a baseline. Surprisingly, this model is already competitive with the state of the art, indicating that there is significant potential for improvement. To explore this potential, we develop a simulation-based throughput predictor. To this end, we propose a detailed parametric pipeline model that supports all Intel Core microarchitecture generations released between 2011 and 2021. We evaluate our predictor on an improved version of the {BHive} benchmark suite and show that its predictions are usually within 1\% of measurement results, improving upon prior models by roughly an order of magnitude. The experimental evaluation also demonstrates that several microarchitectural details considered to be rather insignificant in previous work, are in fact essential for accurate prediction. Our throughput predictor is available as open source.},
	pages = {1--14},
	booktitle = {Proceedings of the 36th {ACM} International Conference on Supercomputing},
	publisher = {Association for Computing Machinery},
	author = {Abel, Andreas and Reineke, Jan},
	urldate = {2023-02-08},
	date = {2022-06-28},
	keywords = {benchmarking, optimization, performance, pipeline model, simulation, throughput prediction},
}


@misc{sykora_granite_2022,
	title = {{GRANITE}: {A} {Graph} {Neural} {Network} {Model} for {Basic} {Block} {Throughput} {Estimation}},
	shorttitle = {{GRANITE}},
	url = {http://arxiv.org/abs/2210.03894},
	doi = {10.48550/arXiv.2210.03894},
	abstract = {Analytical hardware performance models yield swift estimation of desired hardware performance metrics. However, developing these analytical models for modern processors with sophisticated microarchitectures is an extremely laborious task and requires a firm understanding of target microarchitecture's internal structure. In this paper, we introduce GRANITE, a new machine learning model that estimates the throughput of basic blocks across different microarchitectures. GRANITE uses a graph representation of basic blocks that captures both structural and data dependencies between instructions. This representation is processed using a graph neural network that takes advantage of the relational information captured in the graph and learns a rich neural representation of the basic block that allows more precise throughput estimation. Our results establish a new state-of-the-art for basic block performance estimation with an average test error of 6.9\% across a wide range of basic blocks and microarchitectures for the x86-64 target. Compared to recent work, this reduced the error by 1.7\% while improving training and inference throughput by approximately 3.0x. In addition, we propose the use of multi-task learning with independent multi-layer feed forward decoder networks. Our results show that this technique further improves precision of all learned models while significantly reducing per-microarchitecture training costs. We perform an extensive set of ablation studies and comparisons with prior work, concluding a set of methods to achieve high accuracy for basic block performance estimation.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Sykora, Ondrej and Phothilimthana, Phitchaya Mangpo and Mendis, Charith and Yazdanbakhsh, Amir},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03894 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Performance},
}


@article{ritter_anica_2022,
	title = {{AnICA}: analyzing inconsistencies in microarchitectural code analyzers},
	volume = {6},
	shorttitle = {{AnICA}},
	url = {https://dl.acm.org/doi/10.1145/3563288},
	doi = {10.1145/3563288},
	abstract = {Microarchitectural code analyzers, i.e., tools that estimate the throughput of machine code basic blocks, are important utensils in the tool belt of performance engineers. Recent tools like llvm-mca, uiCA, and Ithemal use a variety of techniques and different models for their throughput predictions. When put to the test, it is common to see these state-of-the-art tools give very different results. These inconsistencies are either errors, or they point to different and rarely documented assumptions made by the tool designers. In this paper, we present AnICA, a tool taking inspiration from differential testing and abstract interpretation to systematically analyze inconsistencies among these code analyzers. Our evaluation shows that AnICA can summarize thousands of inconsistencies in a few dozen descriptions that directly lead to high-level insights into the different behavior of the tools. In several case studies, we further demonstrate how AnICA automatically finds and characterizes known and unknown bugs in llvm-mca, as well as a quirk in AMD's Zen microarchitectures.},
	number = {OOPSLA2},
	urldate = {2023-03-19},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {Ritter, Fabian and Hack, Sebastian},
	month = oct,
	year = {2022},
	keywords = {Abstraction, Basic Blocks, Differential Testing, Throughput Prediction},
	pages = {125:1--125:29},
}


@inproceedings{chen_bhive_2019,
	title = {{BHive}: {A} {Benchmark} {Suite} and {Measurement} {Framework} for {Validating} x86-64 {Basic} {Block} {Performance} {Models}},
	shorttitle = {{BHive}},
	doi = {10.1109/IISWC47752.2019.9042166},
	abstract = {Compilers and performance engineers use hardware performance models to simplify program optimizations. Performance models provide a necessary abstraction over complex modern processors. However, constructing and maintaining a performance model can be onerous, given the numerous microarchi-tectural optimizations employed by modern processors. Despite their complexity and reported inaccuracy (e.g., deviating from native measurement by more than 30\%), existing performance models-such as IACA and llvm-mca-have not been systematically validated, because there is no scalable machine code profiler that can automatically obtain throughput of arbitrary basic blocks while conforming to common modeling assumptions. In this paper, we present a novel profiler that can profile arbitrary memory-accessing basic blocks without any user intervention. We used this profiler to build BHive, a benchmark for systematic validation of performance models of x86-64 basic blocks. We used BHive to evaluate four existing performance models: IACA, llvm-mca, Ithemal, and OSACA. We automatically cluster basic blocks in the benchmark suite based on their utilization of CPU resources. Using this clustering, our benchmark can give a detailed analysis of a performance model's strengths and weaknesses on different workloads (e.g., vectorized vs. scalar basic blocks). We additionally demonstrate that our dataset well captures basic properties of two Google applications: Spanner and Dremel.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Chen, Yishen and Brahmakshatriya, Ajay and Mendis, Charith and Renda, Alex and Atkinson, Eric and Sýkora, Ondřej and Amarasinghe, Saman and Carbin, Michael},
	month = nov,
	year = {2019},
	keywords = {Benchmarking, Cost/performance, Measurement techniques, Modeling techniques},
	pages = {167--177},
}

